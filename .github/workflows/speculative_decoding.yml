name: speculative_decoding_lm
on:
  pull_request:
    paths:
      - .github/workflows/speculative_decoding_lm.yml
      - llm_bench/python/**
      - text_generation/causal_lm/cpp/*
      - thirdparty/openvino_tokenizers
      - '!**.md'
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
jobs:
  cpp-speculative_decoding_lm-ubuntu:
    runs-on: ubuntu-20.04-8-cores
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive
      - uses: actions/setup-python@v4
        with:
          python-version: 3.8
      - name: Install OpenVINO
        run: |
          mkdir ./ov/
          curl https://storage.openvinotoolkit.org/repositories/openvino/packages/2023.3/linux/l_openvino_toolkit_ubuntu20_2023.3.0.13775.ceeafaf64f3_x86_64.tgz | tar --directory ./ov/ --strip-components 1 -xz
          sudo ./ov/install_dependencies/install_openvino_dependencies.sh
      - name: Download, convert and build
        run: |
          source ./ov/setupvars.sh
          python -m pip install --upgrade-strategy eager "optimum>=1.14" -r ./llm_bench/python/requirements.txt "transformers<4.38" ./thirdparty/openvino_tokenizers/[transformers] --extra-index-url https://download.pytorch.org/whl/cpu
          python ./llm_bench/python/convert.py --model_id TinyLlama/TinyLlama-1.1B-Chat-v1.0 --output_dir ./TinyLlama-1.1B-Chat-v1.0-not-stateful/ --precision FP16 --disable-stateful
          python ./llm_bench/python/convert.py --model_id meta-llama/Llama-2-7b-chat-hf --output_dir ./Llama-2-7b-chat-hf-not-stateful/ --precision FP16 --disable-stateful
          python ./llm_bench/python/convert.py --model_id meta-llama/Llama-2-7b-chat-hf --output_dir ./Llama-2-7b-chat-hf/ --precision FP16
          convert_tokenizer ./TinyLlama-1.1B-Chat-v1.0-not-stateful/pytorch/dldt/FP16/ --output ./TinyLlama-1.1B-Chat-v1.0-not-stateful/pytorch/dldt/FP16/ --with-detokenizer
          cmake -DCMAKE_BUILD_TYPE=Release -S ./text_generation/causal_lm/cpp/ -B ./build/
          cmake --build ./build/ --config Release -j
          wait
      - name: run and compare
        run: |
          source ./ov/setupvars.sh
          timeout 120s ./build/speculative_decoding_lm ./TinyLlama-1.1B-Chat-v1.0-not-stateful/pytorch/dldt/FP16/ ./Llama-2-7b-chat-hf-not-stateful/pytorch/dldt/FP16/ "Alan Turing was a" > predictions_speculative.txt
          timeout 120s ./build/greedy_causal_lm ./Llama-2-7b-chat-hf/pytorch/dldt/FP16/ "Alan Turing was a" > predictions_greedy.txt
          python -c "
          with open('predictions_greedy.txt', 'r') as f:
              predicted_greedy = f.readline()
          with open('predictions_speculative.txt', 'r') as f:
              predicted_speculative = f.readline()
          assert predicted_greedy == predicted_speculative
          "
          echo speculative_decoding_lm passed
  